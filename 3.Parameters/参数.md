# 参数

## 可学习参数和超参数

**可学习参数**

<img src="https://latex.codecogs.com/gif.latex?W,b">

例如 一元一次方程<img src="https://latex.codecogs.com/gif.latex?y&space;=&space;Wx&space;&plus;&space;b"> 中的 <img src="https://latex.codecogs.com/gif.latex?W,b"> 就是根据训练集自学习的参数。在神经网络中，<img src="https://latex.codecogs.com/gif.latex?W"> 通常表示权重向量<img src="https://latex.codecogs.com/gif.latex?[w_1,&space;w_2...,w_n]"> ，<img src="https://latex.codecogs.com/gif.latex?b"> 通常表示偏置。

**超参数**

- 学习率 <img src="https://latex.codecogs.com/gif.latex?\alpha"> 
- 迭代次数
- 神经网络层数 <img src="https://latex.codecogs.com/gif.latex?L"> 
- 隐藏层每一层的单元个数
- 激活函数
- 动量法的参数
- 小批量梯度下降优化算法的批大小
- 正则化参数

> 神经网络采用的是小批量梯度下降优化算法。动量法是梯度下降方向优化的方法。

## 参数初始化

**小值初始化**

在初始化参数<img src="https://latex.codecogs.com/gif.latex?W"> 时，通常将其初始化为比较小的值。比如在Python中这样实现：
``` 
W = numpy.random.randn(shape) * 0.01
```

进行小值初始化的原因是，当使用的激活函数为Sigmoid时，如果权重过大，在进行反向传播计算时会导致梯度很小，可能引起梯度消失问题。

**结合网络单元数的小值的权重初始化**

同样，我们使用伪代码的方法表示各种初始化的工作方式。当隐藏层网络单元的个数很大时，更加倾向于使用较小的值进行权重初始化，防止训练时梯度消失或梯度爆炸。如下图这样：

![结合网络单元数的小值的权重初始化](https://createmomo.github.io/2018/01/23/Super-Machine-Learning-Revision-Notes/weight_init.png)

基于上述的思路，可以结合隐藏层单元的个数对权重进行初始化，Python表达如下：
```
W = numpy.random.randn(shape) * numpy.sqrt(1/n[l-1])
```
其对应的数学表达式为：<img src="https://latex.codecogs.com/gif.latex?\sqrt&space;{\frac&space;{1&space;}{n^{l-1}}}"> ,<img src="https://latex.codecogs.com/gif.latex?n^{l-1}"> 表示第<img src="https://latex.codecogs.com/gif.latex?l-1"> 层的神经元个数，如果选用的是ReLU激活函数，对应的数学表达式为：<img src="https://latex.codecogs.com/gif.latex?\sqrt&space;{\frac&space;{2&space;}{n^{l-1}}}"> 。



**Xavier初始化**

如果在神经网络中你使用的激活函数是tanh，使用Xavier进行权重初始化能够取得不错的效果，Xavier的公式如下：<img src="https://latex.codecogs.com/gif.latex?\sqrt&space;{&space;\frac{1&space;}{&space;n^{l-1}}&space;}"> 或者<img src="https://latex.codecogs.com/gif.latex?\sqrt&space;{&space;\frac{&space;2&space;}{&space;n^{l-1}&space;&plus;&space;n^l}&space;}"> （其中<img src="https://latex.codecogs.com/gif.latex?n^{l-1}"> 表示第 <img src="https://latex.codecogs.com/gif.latex?l-1">层的神经元个数，<img src="https://latex.codecogs.com/gif.latex?n^l">表示第<img src="https://latex.codecogs.com/gif.latex?l">层的神经元个数）。

> 不同文献中 Xavier初始化的表达式不同，但大同小异，改变的只是根号下分子部分，最终不会改变参数的分布。

## 超参数调优
当训练超参数时，尝试参数所有的可能取值是必要的，如果在资源允许的情况下把不同的参数值传给同一个模型进行并行训练是最简单的方法。但是事实上，资源是很有限的，在这种情况下，同一时间，我们只能训练一个模型，并在不同时间尝试不同参数。
![单独训练模型和并行训练模型](https://createmomo.github.io/2018/01/23/Super-Machine-Learning-Revision-Notes/hyper_parameter_tuning.png)

除了上述介绍的，如何选择合适的超参数也是非常重要的。

在神经网络中都很多超参数，比如：学习率<img src="https://latex.codecogs.com/gif.latex?\alpha"> ，动量法和RMSprop的参数(<img src="https://latex.codecogs.com/gif.latex?\beta&space;_1,\beta&space;_2,\epsilon">)，神经网络层数，每一层的单元数，学习率衰减参数，批训练的批大小。

Andrew Ng提出了相关参数的优先级，如下：

优先级 | 超参数
--- | ---
1 | 学习率<img src="https://latex.codecogs.com/gif.latex?\alpha">
2 | <img src="https://latex.codecogs.com/gif.latex?\beta&space;_1,\beta&space;_2,\epsilon"> (动量法和RMSprop的参数)
2 | 隐藏层单元数
2 | 批训练的批大小
3 | 网络层数
3 | 学习率衰减系数

通常默认的动量法和RMSprop的参数为：<img src="https://latex.codecogs.com/gif.latex?\beta&space;_1&space;=&space;0.9,&space;\beta&space;_2&space;=&space;0.99,\epsilon&space;=&space;10^{-8}">

**隐藏层和网络单元的均匀采样**

例如，神经网络层范围是[2,6]，我们可以均匀的尝试2，3，4，5，6去训练模型。同样网络单元范围是[50,100]，在这个范围内进行尝试也是一个好的策略。表示如下：

![隐藏层和单元](https://createmomo.github.io/2018/01/23/Super-Machine-Learning-Revision-Notes/hyper_parameter_tuning_units_and_layers.png)

**对数采样**

或许，你已经意识到均匀采样对于所有类型的参数不是一个好方法。

例如，我们认为学习率<img src="https://latex.codecogs.com/gif.latex?\alpha&space;\in&space;[0.0001,1]&space;=&space;[10^{-4},10^0]"> 是一个合适的范围。很显然，均匀采样是不合理的，一种更合适的方法是进行对数采样，<img src="https://latex.codecogs.com/gif.latex?\alpha=&space;10^r,r\in&space;[-4,0]&space;(0.0001,&space;0.001,&space;0.01,&space;0.1,1)"> 。

对于参数<img src="https://latex.codecogs.com/gif.latex?\beta&space;_1,&space;\beta&space;_2"> ，可以采用相同的策略。

例如：<img src="https://latex.codecogs.com/gif.latex?1-&space;\beta=&space;10&space;^r"> ，因此<img src="https://latex.codecogs.com/gif.latex?\beta&space;=&space;1-&space;10^r"> , <img src="https://latex.codecogs.com/gif.latex?r&space;\in&space;[-3,-1]">

下面的这个表格，能够帮你更好的理解这种策略

<img src="https://latex.codecogs.com/gif.latex?\beta"> | 0.9 | 0.99 | 0.999
--- | ---| --- | ---
<img src="https://latex.codecogs.com/gif.latex?1&space;-&space;\beta"> | 0.1 | 0.01 | 0.001
<img src="https://latex.codecogs.com/gif.latex?r">  | -1 | -2 | -3

例如：

![学习率](https://createmomo.github.io/2018/01/23/Super-Machine-Learning-Revision-Notes/hyper_parameter_tuning_alpha_and_beta.png)
