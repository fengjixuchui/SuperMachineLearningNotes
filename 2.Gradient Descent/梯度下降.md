# 梯度下降
梯度下降是求解目标函数局部最小值的一种迭代方法（如损失函数），其迭代的过程如下：
```
Repeat{
    W := W - learning_rate * dJ(W)/dW
}
```
符号<img src="https://latex.codecogs.com/gif.latex?:="> 表示覆盖操作。从公式中可以看出，在梯度下降求解过程中要不断的去更新<img src="https://latex.codecogs.com/gif.latex?W">

通常用<img src="https://latex.codecogs.com/gif.latex?\alpha"> 表示学习率，当训练神经网络时，它是一个很重要的超参数（更多关于超参数的介绍可参考下一节）。<img src="https://latex.codecogs.com/gif.latex?J(W)"> 表示模型的损失函数，<img src="https://latex.codecogs.com/gif.latex?\frac{d&space;J(W)}{d(W)}"> 是关于参数<img src="https://latex.codecogs.com/gif.latex?W">的梯度，如果参数<img src="https://latex.codecogs.com/gif.latex?W">是个矩阵，则<img src="https://latex.codecogs.com/gif.latex?\frac{d&space;J(W)}{d(W)}"> 也会是一个矩阵。

**问题：为什么我们在最小化损失函数时不是加上梯度？**

答：假设损失函数是<img src="https://latex.codecogs.com/gif.latex?J(W)=0.1&space;(W-5)^2"> ，如下图所示：

![损失函数曲线图](https://createmomo.github.io/2018/01/23/Super-Machine-Learning-Revision-Notes/gradient_descent_smaller.png)

当参数<img src="https://latex.codecogs.com/gif.latex?W=10"> 时，梯度<img src="https://latex.codecogs.com/gif.latex?\frac{d&space;J(W)}{d(W)}&space;=&space;0.1&space;*&space;2&space;(10-5)&space;=&space;1"> ，很显然，如果继续寻找最小的损失函数<img src="https://latex.codecogs.com/gif.latex?J(W)"> 时，梯度的反方向（eg:<img src="https://latex.codecogs.com/gif.latex?-\frac{d&space;J(W)}{d(W)}"> ）是找到局部最优点的正确方向（eg：<img src="https://latex.codecogs.com/gif.latex?J(W=5)=0"> ）。

但需要注意的是，梯度下降法有时候会遇到局部最优解问题。


## 计算图

计算图的例子在[Deep Learning AI](https://www.deeplearning.ai/)的第一节课程中被提到。

假设有三个可学习的参数<img src="https://latex.codecogs.com/gif.latex?a,b,c"> ，目标函数定位为：<img src="https://latex.codecogs.com/gif.latex?J=3(a&space;&plus;&space;bc)"> ，接下来我们要计算参数的梯度：<img src="https://latex.codecogs.com/gif.latex?\frac&space;{dJ}{da},\frac&space;{dJ}{db},\frac&space;{dJ}{dc}"> ，同时定义<img src="https://latex.codecogs.com/gif.latex?u&space;=&space;bc,v&space;=&space;a&plus;u,J&space;=3v"> ，则计算过程可以转化为下边这样的计算图。

![计算图](https://createmomo.github.io/2018/01/23/Super-Machine-Learning-Revision-Notes/computation_graph-1.png)

## 反向传播算法
从上图可以看出，参数的梯度为：<img src="https://latex.codecogs.com/gif.latex?\frac&space;{dJ}{da}&space;=&space;\frac{dJ}{dv}&space;\frac&space;{dv}{da},&space;\frac&space;{dJ}{db}&space;=\frac&space;{dJ}{dv}\frac&space;{dv}{du}\frac&space;{du}{db},&space;\frac&space;{dJ}{dc}&space;=\frac&space;{dJ}{dv}\frac&space;{dv}{du}\frac&space;{du}{dc}">。

计算每个节点的梯度比较容易，如下所示（这里需要注意的是：如果你要实现自己的算法，梯度可以在正向传播时计算，以节省计算资源和训练时间，因此当反向传播时，无需再次计算每个节点的梯度）。

![每个节点的梯度计算](https://createmomo.github.io/2018/01/23/Super-Machine-Learning-Revision-Notes/computation_graph-2.png)

现在可以通过简单的组合节点梯度来计算每个参数的梯度。

![反向传播](https://createmomo.github.io/2018/01/23/Super-Machine-Learning-Revision-Notes/computation_graph-3.png)


<img src="https://latex.codecogs.com/gif.latex?\frac&space;{dJ}{da}&space;=&space;\frac{dJ}{dv}&space;\frac&space;{dv}{da}&space;=&space;3*&space;1&space;=&space;3">

<img src="https://latex.codecogs.com/gif.latex?\frac&space;{dJ}{db}&space;=\frac&space;{dJ}{dv}\frac&space;{dv}{du}\frac&space;{du}{db}&space;=&space;3&space;*&space;1&space;*&space;2&space;=&space;6">

<img src="https://latex.codecogs.com/gif.latex?\frac&space;{dJ}{dc}&space;=\frac&space;{dJ}{dv}\frac&space;{dv}{du}\frac&space;{du}{dc}&space;=&space;3&space;*&space;1&space;*&space;3&space;=&space;9">


## L2正则修正的梯度（权重衰减）
通过引入 <img src="https://latex.codecogs.com/gif.latex?\frac&space;{\lambda}{m}&space;W"> 改变梯度的值。

```
Repeat{
    W := W - (lambda/m) * W - learning_rate * dJ(W)/dW
}
```

## 梯度消失和梯度爆炸
如果我们定义了一个非常深的神经网络且没有正确初始化权重，可能会遇到梯度消失或梯度爆炸问题（更多关于参数初始化的可以参考：[参数初始化](##参数初始化)。

这里以一个简单的但是深层的神经网络结构为例（同样，这个很棒的例子来自于线上AI课程：[Deep Learning AI](https://www.deeplearning.ai/)）来解释什么是梯度消失，梯度爆炸。

假设神经网络有<img src="https://latex.codecogs.com/gif.latex?L"> 层，为了简单起见，每一层的参数<img src="https://latex.codecogs.com/gif.latex?b^l">为0，所有的激活函数定义为：<img src="https://latex.codecogs.com/gif.latex?g(z)=z"> ，除此之外，每层的连接权重<img src="https://latex.codecogs.com/gif.latex?W^l"> 拥有相同的权重：<img src="https://latex.codecogs.com/gif.latex?W^{[l]}=\left(\begin{array}{cc}&space;1.5&space;&&space;0\\&space;0&space;&&space;1.5&space;\end{array}\right)"> 。

基于上述的简单网络，最终的输出可能为：
<img src="https://latex.codecogs.com/gif.latex?y=W^{[l]}W^{[l-1]}W^{[l-2]}…W^{[3]}W^{[2]}W^{[1]}X">

如果权重<img src="https://latex.codecogs.com/gif.latex?W=1.5>1"> ，<img src="https://latex.codecogs.com/gif.latex?1.5^L"> 将会在一些元素上引起梯度爆炸。同样，如果权重值小于1，将会在一些元素上引起梯度消失。 

**梯度消失和梯度爆炸会使模型训练变得十分困难，因此，正确初始化神经网络的权重十分重要。**

## 小批量梯度下降
如果训练集数据特别大，单个batch在训练时会花费大量的时间，这对开发者而言，跟踪整个训练过程会变得十分困难。在小批量梯度下降中，根据当前批次样本计算损失和梯度，在一定程度上能够解决该问题。

<img src="https://latex.codecogs.com/gif.latex?X"> 代表整个训练集，它被划分成下面这样的多个批次，<img src="https://latex.codecogs.com/gif.latex?m"> 表示的是训练集的样本数。

![样本的下批量划分](https://createmomo.github.io/2018/01/23/Super-Machine-Learning-Revision-Notes/mini-batch.png)

小批量梯度训练的过程如下：
```
For t= (1, ... , #批次大小):
    基于第t个批次进行前向传播计算;
    计算第t个批次的损失值;
    基于第t个批次进行反向传播计算，以计算梯度并更新参数.
```
在训练过程中，对比不应用小批量梯度下降和应用小批量梯度下降，前者下降的更加平滑。

![不应用小批量梯度下降和应用小批量梯度下降](https://createmomo.github.io/2018/01/23/Super-Machine-Learning-Revision-Notes/mini_batch_loss.png)


## 随机梯度下降

随机梯度下降时，批次的样本数大小为1。

## 小批量梯度下降批次大小选择
小批量大小：
1. 如果批次大小为M，即整个训练集的样本数，则梯度下降恰好为批量梯度下降
2. 如果批次大小为1，则为随机梯度下降

实际应用中，批次大小是在<img src="https://latex.codecogs.com/gif.latex?[1,M]"> 之间选择。如果<img src="https://latex.codecogs.com/gif.latex?M&space;\leq&space;2000"> ，该数据集是一个小型数据集，使用批量梯度下降是可以接受的。如果<img src="https://latex.codecogs.com/gif.latex?M&space;>&space;2000"> ，使用小批量梯度下降算法训练模型更加适合。通常小批量的大小设置为：64，128，256等。

下图为使用不同批次大小训练模型的下降过程。

![不同批次大小的训练过程](https://createmomo.github.io/2018/01/23/Super-Machine-Learning-Revision-Notes/mini_batch_gradient.png)


## Momentum
> 神经网络中普遍使用的是小批量梯度下降优化算法，因此这里介绍的Momentum和下边介绍的RMSprop，Adma都是结合小批量梯度下降优化进行的。

增加动量法小批量梯度第<img src="https://latex.codecogs.com/gif.latex?t"> 次迭代过程如下：
1. 基于当前的批次数据计算<img src="https://latex.codecogs.com/gif.latex?dW,db"> 
2. <img src="https://latex.codecogs.com/gif.latex?V_{dW}=\beta&space;V_{dW}&plus;(1-\beta)dW">
3. <img src="https://latex.codecogs.com/gif.latex?V_{db}=\beta&space;V_{db}&plus;(1-\beta)db">
4. <img src="https://latex.codecogs.com/gif.latex?W:=W-\alpha&space;V_{dW}">
5. <img src="https://latex.codecogs.com/gif.latex?b:=b-\alpha&space;V_{db}">

动量法中的超参数为<img src="https://latex.codecogs.com/gif.latex?\alpha,&space;\beta"> 。在动量法中，<img src="https://latex.codecogs.com/gif.latex?V_{dW}"> 是上一个批次的历史梯度数据。如果令<img src="https://latex.codecogs.com/gif.latex?\beta=0.9"> ，这意味着要考虑最近10次迭代的梯度以更新参数。

<img src="https://latex.codecogs.com/gif.latex?\beta"> 原本是来自[指数加权平均值](https://www.youtube.com/watch?v=NxTFlzBjS-4)的参数。例如：<img src="https://latex.codecogs.com/gif.latex?\beta=0.9"> 意味着取最近10个值作为平均值，<img src="https://latex.codecogs.com/gif.latex?\beta=0.999"> 意味着考虑最近1000次的结果。

## RMSprop
RMSprop的全称是Root Mean Square Prop，

在RMSprop优化算法下，第<img src="https://latex.codecogs.com/gif.latex?t"> 个批次的迭代过程如下：
1. 基于当前的批次数据计算<img src="https://latex.codecogs.com/gif.latex?dW,db"> 
2. <img src="https://latex.codecogs.com/gif.latex?S_{dW}=\beta&space;S_{dW}&plus;(1-\beta)(dW)^2">
3. <img src="https://latex.codecogs.com/gif.latex?S_{db}=\beta&space;S_{db}&plus;(1-\beta)(db)^2">
4. <img src="https://latex.codecogs.com/gif.latex?W:=W&space;-\alpha&space;\frac{dW}{\sqrt{S_{dW}}&plus;\epsilon}">
5. <img src="https://latex.codecogs.com/gif.latex?b:=b-\alpha&space;\frac{db}{\sqrt{S_{db}}&plus;\epsilon}">


## Adma
> Adma全称是Adaptive Moment Estimation，自适应动量估计算法。可以看作是动量法和RMSprop的结合，不但使用动量作为参数更新，而且可以自适应调整学习率。

<img src="https://latex.codecogs.com/gif.latex?V_{dW}=0,&space;S_{dW=0},V_{db}=0,&space;S_{db}=0">

在Adma优化算法下，第t个批次的迭代过程如下：

1). 基于当前的批次数据计算<img src="https://latex.codecogs.com/gif.latex?dW,db"> 

// 动量法

2). <img src="https://latex.codecogs.com/gif.latex?V_{dW}=\beta_1&space;V_{dW}&plus;(1-\beta_1)dW">

3). <img src="https://latex.codecogs.com/gif.latex?V_{db}=\beta_1&space;V_{db}&plus;(1-\beta_1)db">

// RMSprop

4). <img src="https://latex.codecogs.com/gif.latex?S_{dW}=\beta_2&space;S_{dW}&plus;(1-\beta_2)(dW)^2">

5). <img src="https://latex.codecogs.com/gif.latex?S_{db}=\beta_2&space;S_{db}&plus;(1-\beta_2)(db)^2">

// 偏差校正

6). <img src="https://latex.codecogs.com/gif.latex?V_{dW}^{correct}=\frac{V_{dW}}{1-\beta_1^t}">

7). <img src="https://latex.codecogs.com/gif.latex?V_{db}^{correct}=\frac{V_{db}}{1-\beta_1^t}">

6). <img src="https://latex.codecogs.com/gif.latex?S_{dW}^{correct}=\frac{S_{dW}}{1-\beta_2^t}">

7). <img src="https://latex.codecogs.com/gif.latex?S_{db}^{correct}=\frac{S_{db}}{1-\beta_2^t}">

// 参数更新

<img src="https://latex.codecogs.com/gif.latex?W:=W&space;-\alpha&space;\frac{V_{dW}^{correct}}{\sqrt{S_{dW}^{correct}}&plus;\epsilon}">

<img src="https://latex.codecogs.com/gif.latex?b:=b-\alpha&space;\frac{V_{db}^{correct}}{\sqrt{S_{db}^{correct}}&plus;\epsilon}">

"纠正"是指数加权平均的[偏差校正](https://www.youtube.com/watch?v=lWzo8CajF5s)的概念。纠正使得平均值的计算更加准确。<img src="https://latex.codecogs.com/gif.latex?t"> 是<img src="https://latex.codecogs.com/gif.latex?\beta">的幂。

通常，超参数的值为：<img src="https://latex.codecogs.com/gif.latex?\beta_1&space;=&space;0.9"> ，<img src="https://latex.codecogs.com/gif.latex?\beta_2&space;=&space;0.99"> ，<img src="https://latex.codecogs.com/gif.latex?\epsilon=10^{-8}">

学习率<img src="https://latex.codecogs.com/gif.latex?\alpha"> 需要进行调整的，当然也可以使用学习率衰减的方法，同样可以取得不错的效果。

## 学习率衰减

如果在训练期间固定学习率，如下图所示，损失或者目标可能会波动。因此寻找一种具备自适应调整的学习率会是一个很好的方法。

![学习率衰减](https://createmomo.github.io/2018/01/23/Super-Machine-Learning-Revision-Notes/learning_rate_decay_methods.png)

**基于Epoch的衰减**

![基于Epoch的衰减](https://createmomo.github.io/2018/01/23/Super-Machine-Learning-Revision-Notes/learning_rate_decay_methods_epoch.png)

根据epoch的值来降低学习率是一个直接方法，其衰减方程如下：

<img src="https://latex.codecogs.com/gif.latex?\alpha=\frac{1}{1&plus;DecayRate*EpochNumber}\alpha_0">

其中 DecayRate是衰减率，EpochNumber表示epoch的次数。

例如，初始学习率<img src="https://latex.codecogs.com/gif.latex?\alpha=0.2"> ，衰减率为1.0，每次epoch的学习率为：

Epoch | <img src="https://latex.codecogs.com/gif.latex?\alpha">
----| ---
1 | 0.1
2 | 0.67
3 | 0.5
4 | 0.4
5 | ...

也有一些其他的学习率衰减方法，如下：

方法 | 表达式
--- | ---
指数衰减 | <img src="https://latex.codecogs.com/gif.latex?\alpha=0.95^{EpochNumber}\alpha_0">
基于epoch次数的衰减 | <img src="https://latex.codecogs.com/gif.latex?\alpha=\frac{k}{EpochNumber}\alpha_0">
基于批量大小的衰减 | <img src="https://latex.codecogs.com/gif.latex?\alpha=\frac{k}{t}\alpha_0">
“楼梯”衰减 | ![“楼梯”衰减](https://createmomo.github.io/2018/01/23/Super-Machine-Learning-Revision-Notes/discrete_stair_case.png)
手动衰减 | 按照天或者小时手动衰减降低学习率




## 批量归一化

**训练时批量归一化**

批量标准化可以加快训练速度，步骤如下：

![批量归一化](https://createmomo.github.io/2018/01/23/Super-Machine-Learning-Revision-Notes/batch_normalization.png)

每个层<img src="https://latex.codecogs.com/gif.latex?l"> 中，归一化的具体公式如下：

<img src="https://latex.codecogs.com/gif.latex?\mu=\frac{1}{m}\sum&space;Z^{(i)}">

<img src="https://latex.codecogs.com/gif.latex?\delta^2=\frac{1}{m}\sum&space;(Z^{(i)}-\mu)">

<img src="https://latex.codecogs.com/gif.latex?Z^{(i)}_{normalized}=\alpha&space;\frac{Z^{(i)}\mu}{\sqrt{\delta^2}&plus;\epsilon}&space;&plus;\beta">

<img src="https://latex.codecogs.com/gif.latex?\alpha,&space;\beta"> 是可学习的参数。

**测试时批量归一化**

在测试时，因为每次可能只有一个测试的实例样本，所以没有充足的实例样本计算<img src="https://latex.codecogs.com/gif.latex?\mu"> 和 <img src="https://latex.codecogs.com/gif.latex?\delta"> 。

在这种情况下，最好使用跨批量的指数加权平均值来估计<img src="https://latex.codecogs.com/gif.latex?\mu"> 和 <img src="https://latex.codecogs.com/gif.latex?\delta"> 的合理值。